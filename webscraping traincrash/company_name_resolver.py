#!/usr/bin/env python3
"""
Company Name Resolver
=====================

PROBLEMET:
- Vi har f√∂retagsnamn i v√•r databas som "Arlaplast"
- Men i SCB heter det "Arla Plast AB"
- SCB API ger ingen tr√§ff

L√ñSNINGEN:
- G√• in p√• f√∂retagets egen webbsajt
- Hitta deras OFFICIELLA f√∂retagsnamn (juridiskt namn)
- Hitta deras organisationsnummer (om det finns p√• sajten)
- Anv√§nd det f√∂r korrekt SCB-s√∂kning

STRATEGI:
1. Kolla f√∂retagets webbsajt (footer, Om oss, Kontakt)
2. Leta efter org.nr (vanligt i footer)
3. Leta efter officiellt f√∂retagsnamn (ofta "¬© 2024 F√∂retag AB")
4. Exportera mappning: ditt_namn ‚Üí officiellt_namn + org.nr
"""

import argparse
import csv
import re
import sqlite3
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# ============================================================================
# KONFIGURATION
# ============================================================================

DB_PATH = Path(__file__).parent.parent / "databases" / "ai_companies.db"
RATE_LIMIT_DELAY = 1.0
TIMEOUT = 10

# Sidor att kolla (i prioritetsordning)
PAGES_TO_CHECK = [
    "",  # Huvudsida
    "/om-oss",
    "/about",
    "/about-us",
    "/kontakt",
    "/contact",
    "/foretaget",
    "/company",
]

# ============================================================================
# ORG.NR EXTRAKTION
# ============================================================================

def extract_orgnr_from_text(text: str) -> Optional[str]:
    """
    Hitta org.nr i text
    
    Vanliga format p√• f√∂retags egna sajter:
    - Organisationsnummer: 556498-5025
    - Org.nr: 556498-5025
    - Org nr 5564985025
    - ¬© 2024 F√∂retag AB (556498-5025)
    """
    if not text:
        return None

    patterns = [
        # Med label "Organisationsnummer" eller "Org.nr"
        r'[Oo]rg(?:anisations)?\.?\s*[Nn]r\.?\s*[:\-]?\s*(\d{6}[-\s]?\d{4})',
        # I parentes (vanligt i footer)
        r'\((\d{6}[-\s]?\d{4})\)',
        # Standard format
        r'\b(\d{6}-\d{4})\b',
        # 10 siffror i rad (b√∂rjar med 5)
        r'\b(5\d{9})\b',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            orgnr = match.group(1).replace(' ', '').replace('-', '')
            if len(orgnr) == 10 and orgnr[0] == '5':
                return f"{orgnr[:6]}-{orgnr[6:]}"
    
    return None


# ============================================================================
# F√ñRETAGSNAMN EXTRAKTION
# ============================================================================

def extract_official_company_name(soup: BeautifulSoup, url: str) -> List[str]:
    """
    F√∂rs√∂k hitta f√∂retagets officiella namn p√• sajten
    
    Strategier:
    1. Copyright text i footer (¬© 2024 F√∂retag AB)
    2. "Om oss" headings
    3. Meta tags (og:site_name, etc)
    4. Address/vCard markup
    
    Returns:
        Lista med kandidater (kan vara flera)
    """
    candidates = []
    
    # 1. COPYRIGHT I FOOTER
    # Leta efter ¬© f√∂ljt av f√∂retagsnamn
    copyright_patterns = [
        r'¬©\s*\d{4}\s+([^.\n]+?(?:AB|Aktiebolag|HB|KB))',
        r'[Cc]opyright\s*\d{4}\s+([^.\n]+?(?:AB|Aktiebolag|HB|KB))',
    ]
    
    footer = soup.find('footer')
    if footer:
        footer_text = footer.get_text()
        for pattern in copyright_patterns:
            matches = re.findall(pattern, footer_text)
            candidates.extend(matches)
    
    # 2. META TAGS
    meta_tags = [
        ('property', 'og:site_name'),
        ('name', 'og:site_name'),
        ('property', 'twitter:site'),
    ]
    
    for attr_name, attr_value in meta_tags:
        meta = soup.find('meta', {attr_name: attr_value})
        if meta and meta.get('content'):
            candidates.append(meta['content'])
    
    # 3. STRUCTURED DATA (JSON-LD)
    # Leta efter Organization schema
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            import json
            data = json.loads(script.string)
            if isinstance(data, dict):
                if data.get('@type') == 'Organization':
                    if data.get('legalName'):
                        candidates.append(data['legalName'])
                    elif data.get('name'):
                        candidates.append(data['name'])
        except:
            pass
    
    # 4. HEADINGS P√Ö "OM OSS"-SIDA
    # Om URL:en inneh√•ller "om" eller "about"
    if any(word in url.lower() for word in ['/om', '/about', '/foretag', '/company']):
        h1 = soup.find('h1')
        if h1:
            candidates.append(h1.get_text(strip=True))
    
    # 5. COMPANY NAME I ADDRESS/VCARD
    # Leta efter mikroformat eller adresser
    addresses = soup.find_all(['address', 'div'], class_=re.compile('vcard|company|organization'))
    for addr in addresses:
        text = addr.get_text()
        # Leta efter n√•got som slutar p√• AB, Aktiebolag etc
        match = re.search(r'([A-Z√Ö√Ñ√ñ][a-z√•√§√∂\s]+(?:AB|Aktiebolag|HB|KB))', text)
        if match:
            candidates.append(match.group(1))
    
    # Rensa och normalisera kandidater
    cleaned = []
    for c in candidates:
        c = c.strip()
        # Ta bort extra whitespace
        c = re.sub(r'\s+', ' ', c)
        # M√•ste inneh√•lla minst 3 tecken
        if len(c) >= 3:
            # Ta bort vanliga suffixer som inte h√∂r till namnet
            c = re.sub(r'\s*\|.*$', '', c)
            cleaned.append(c)
    
    # Ta bort dubbletter, beh√•ll ordning
    seen = set()
    unique = []
    for c in cleaned:
        if c.lower() not in seen:
            seen.add(c.lower())
            unique.append(c)
    
    return unique


# ============================================================================
# WEB SCRAPING
# ============================================================================

def scrape_company_website(website: str) -> Dict[str, any]:
    """
    Scrapa f√∂retagets webbsajt f√∂r att hitta officiellt namn och org.nr

    Returns:
        {
            'official_names': [lista med kandidater],
            'orgnr': 'XXXXXX-XXXX' eller None,
            'found_on_page': 'URL d√§r info hittades',
            'error': None eller felmeddelande
        }
    """
    result = {
        'official_names': [],
        'orgnr': None,
        'found_on_page': None,
        'error': None
    }

    # Normalisera URL
    if not website:
        result['error'] = "Ingen webbsajt angiven"
        return result

    if not website.startswith('http'):
        website = 'https://' + website

    # Ta bort trailing slash
    website = website.rstrip('/')

    # Skapa en session f√∂r att beh√•lla cookies
    session = requests.Session()

    # Realistiska headers f√∂r att undvika blockering
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'sv-SE,sv;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
    }

    # F√∂rs√∂k olika sidor
    for page_path in PAGES_TO_CHECK:
        url = website + page_path

        try:
            response = session.get(
                url,
                timeout=TIMEOUT,
                headers=headers,
                allow_redirects=True
            )
            
            # Om 404, testa n√§sta sida
            if response.status_code == 404:
                continue
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Leta efter org.nr
            page_text = soup.get_text()
            orgnr = extract_orgnr_from_text(page_text)
            
            # Leta efter f√∂retagsnamn
            names = extract_official_company_name(soup, url)
            
            # Om vi hittade n√•got, spara det
            if orgnr or names:
                if orgnr and not result['orgnr']:
                    result['orgnr'] = orgnr
                    result['found_on_page'] = url
                
                if names:
                    # L√§gg till nya unika namn
                    for name in names:
                        if name not in result['official_names']:
                            result['official_names'].append(name)
                
                # Om vi hittade org.nr, sluta s√∂ka
                if result['orgnr']:
                    break
            
            # V√§nta lite mellan sidorna
            time.sleep(0.3)
        
        except requests.exceptions.Timeout:
            result['error'] = f"Timeout vid h√§mtning av {url}"
            continue
        except requests.exceptions.ConnectionError:
            result['error'] = f"Kunde inte ansluta till {url}"
            continue
        except Exception as e:
            result['error'] = f"Fel vid scraping: {str(e)}"
            continue
    
    # Om vi inte hittade n√•got alls
    if not result['official_names'] and not result['orgnr']:
        if not result['error']:
            result['error'] = "Hittade inget f√∂retagsnamn eller org.nr p√• webbsajten"
    
    return result


# ============================================================================
# DATABASE
# ============================================================================

def get_companies_without_scb(limit: Optional[int] = None) -> List[Tuple]:
    """H√§mta f√∂retag som saknar SCB-data OCH har en webbsajt"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()

    query = """
        SELECT c.id, c.name, c.type, c.website
        FROM companies c
        LEFT JOIN scb_enrichment se ON c.id = se.company_id
        WHERE se.company_id IS NULL
        AND c.website IS NOT NULL
        AND c.website != ''
        ORDER BY c.name
    """

    if limit:
        query += " LIMIT ?"
        cursor.execute(query, [limit])
    else:
        cursor.execute(query)
    
    results = cursor.fetchall()
    conn.close()

    return results


def process_company(company_id: int, name: str, company_type: str, website: str) -> Dict:
    """Processa ett f√∂retag"""
    print(f"\n{'='*70}")
    print(f"üè¢ {name}")
    print(f"   Webbsajt: {website}")
    
    # Scrapa webbsajten
    scrape_result = scrape_company_website(website)
    
    result = {
        'company_id': company_id,
        'company_name': name,
        'company_type': company_type,
        'website': website,
        'found_orgnr': scrape_result['orgnr'] or '',
        'official_name_1': '',
        'official_name_2': '',
        'official_name_3': '',
        'found_on_page': scrape_result['found_on_page'] or '',
        'error': scrape_result['error'] or '',
        'notes': ''
    }
    
    # Fyll i upp till 3 namnkandidater
    names = scrape_result['official_names']
    if len(names) >= 1:
        result['official_name_1'] = names[0]
    if len(names) >= 2:
        result['official_name_2'] = names[1]
    if len(names) >= 3:
        result['official_name_3'] = names[2]
    
    # Skriv ut resultat
    if scrape_result['orgnr']:
        print(f"   ‚úÖ Org.nr: {scrape_result['orgnr']}")
    
    if scrape_result['official_names']:
        print(f"   üìù Officiella namn hittade:")
        for i, n in enumerate(scrape_result['official_names'][:3], 1):
            print(f"      {i}. {n}")
    
    if scrape_result['error']:
        print(f"   ‚ö†Ô∏è  {scrape_result['error']}")
    
    return result


def export_to_csv(results: List[Dict], output_path: Path):
    """Exportera resultat till CSV"""
    if not results:
        print("‚ö†Ô∏è  Inga resultat att exportera")
        return

    fieldnames = [
        'company_id',
        'company_name',
        'company_type',
        'website',
        'found_orgnr',
        'official_name_1',
        'official_name_2',
        'official_name_3',
        'found_on_page',
        'error',
        'notes'
    ]

    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"\n‚úÖ Exporterat till: {output_path}")


# ============================================================================
# MAIN
# ============================================================================

def parse_args():
    parser = argparse.ArgumentParser(
        description="Hitta f√∂retags officiella namn genom att scrapa deras egna webbsajter"
    )
    parser.add_argument("--limit", type=int, help="Max antal f√∂retag att processa")
    parser.add_argument("--output", type=str, help="Output CSV-fil")
    parser.add_argument("--yes", "-y", action="store_true", help="Hoppa √∂ver bekr√§ftelse")
    return parser.parse_args()


def main():
    args = parse_args()

    print("="*70)
    print("üîç COMPANY NAME RESOLVER")
    print("="*70)
    print("\nHittar f√∂retagens OFFICIELLA namn fr√•n deras egna webbsajter")
    print("Detta hj√§lper dig matcha mot SCB:s databas korrekt!\n")

    # H√§mta f√∂retag
    companies = get_companies_without_scb(limit=args.limit)
    
    print(f"üìä Hittade {len(companies)} f√∂retag med webbsajt men utan SCB-data")
    
    if not companies:
        print("\nInga f√∂retag att processa.")
        return

    # Bekr√§ftelse
    if not args.yes:
        estimated_time = len(companies) * 3  # ~3 sekunder per f√∂retag
        print(f"\n‚è±Ô∏è  Uppskattat tid: ~{estimated_time / 60:.0f} minuter")
        
        response = input("\nForts√§tta? (y/n): ")
        if response.lower() != 'y':
            print("Avbryter.")
            return

    print(f"\nüöÄ Startar scraping av {len(companies)} webbsajter...\n")

    # Processa varje f√∂retag
    results = []
    for i, (company_id, name, company_type, website) in enumerate(companies, 1):
        print(f"[{i}/{len(companies)}]", end=" ")
        
        try:
            result = process_company(company_id, name, company_type, website)
            results.append(result)
            
            # Rate limiting
            time.sleep(RATE_LIMIT_DELAY)
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Avbrutet av anv√§ndaren")
            break
        except Exception as e:
            print(f"\n‚ùå Ov√§ntat fel: {e}")
            continue

    # Exportera
    if results:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = Path(args.output or f"company_names_resolved_{timestamp}.csv")
        export_to_csv(results, output_path)

        # Statistik
        with_orgnr = sum(1 for r in results if r['found_orgnr'])
        with_names = sum(1 for r in results if r['official_name_1'])
        
        print(f"\nüìä Statistik:")
        print(f"   Totalt processade: {len(results)}")
        print(f"   Hittade org.nr: {with_orgnr} ({with_orgnr/len(results)*100:.0f}%)")
        print(f"   Hittade officiellt namn: {with_names} ({with_names/len(results)*100:.0f}%)")
        
        print(f"\nüí° N√§sta steg:")
        print(f"   1. √ñppna {output_path}")
        print(f"   2. Granska 'official_name_1/2/3' kolumnerna")
        print(f"   3. V√§lj r√§tt officiellt namn f√∂r varje f√∂retag")
        print(f"   4. Anv√§nd det namnet f√∂r att s√∂ka i SCB API")

    print("\n‚úÖ Klart!")


if __name__ == "__main__":
    main()
