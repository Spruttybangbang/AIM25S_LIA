#!/usr/bin/env python3
"""
Script f√∂r att skrapa text fr√•n f√∂retagshemsidor.

L√§ser f√∂retag fr√•n ai_companies.db som har website och:
1. Bes√∂ker hemsidan
2. Extraherar huvudinneh√•ll (text)
3. Sparar till CSV f√∂r vidare bearbetning

Anv√§ndning:
    python3 scripts/scrape_company_websites.py
    python3 scripts/scrape_company_websites.py --limit 10  # Testa p√• 10 f√∂retag f√∂rst
    python3 scripts/scrape_company_websites.py --missing-only  # Bara f√∂retag utan description
"""

import sqlite3
import requests
import csv
from datetime import datetime
from bs4 import BeautifulSoup
import argparse
import sys
import time
from pathlib import Path
from urllib.parse import urlparse
import re

# Timeout f√∂r HTTP requests
HTTP_TIMEOUT = 15

# Headers f√∂r att se ut som en riktig browser
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'sv-SE,sv;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}


def clean_text(text):
    """
    Rensa och normalisera text.
    """
    # Ta bort extra whitespace
    text = re.sub(r'\s+', ' ', text)
    # Ta bort extra newlines
    text = re.sub(r'\n+', '\n', text)
    return text.strip()


def extract_meta_description(soup):
    """
    F√∂rs√∂k h√§mta meta description fr√•n HTML.
    """
    meta_tags = [
        soup.find('meta', attrs={'name': 'description'}),
        soup.find('meta', attrs={'property': 'og:description'}),
        soup.find('meta', attrs={'name': 'twitter:description'})
    ]

    for tag in meta_tags:
        if tag and tag.get('content'):
            return clean_text(tag.get('content'))

    return None


def extract_main_content(soup):
    """
    Extrahera huvudinneh√•ll fr√•n HTML.
    F√∂rs√∂ker hitta huvudtext och undvika navigation, footer, etc.
    """
    # Ta bort script, style, nav, footer
    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
        tag.decompose()

    # Prioritera main-taggar eller article-taggar
    main_content = soup.find('main') or soup.find('article')

    if main_content:
        text = main_content.get_text(separator=' ', strip=True)
    else:
        # Fallback: ta allt fr√•n body
        body = soup.find('body')
        if body:
            text = body.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)

    # Rensa texten
    text = clean_text(text)

    # Begr√§nsa till rimlig l√§ngd (max 5000 tecken)
    if len(text) > 5000:
        text = text[:5000] + '...'

    return text


def scrape_website(url, company_name):
    """
    Skrapa text fr√•n en hemsida.

    Returns:
        dict: {
            'scraped_text': str,
            'meta_description': str or None,
            'status': str,
            'status_code': int or None
        }
    """
    print(f"\nüåê Skrapar: {url}")

    # S√§kerst√§ll att URL har protokoll
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    result = {
        'scraped_text': '',
        'meta_description': None,
        'status': 'unknown',
        'status_code': None
    }

    try:
        # F√∂rs√∂k f√∂rst med HTTPS
        response = requests.get(
            url,
            headers=HEADERS,
            timeout=HTTP_TIMEOUT,
            allow_redirects=True
        )

        result['status_code'] = response.status_code

        if response.status_code == 200:
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')

            # H√§mta meta description
            meta_desc = extract_meta_description(soup)
            result['meta_description'] = meta_desc

            # H√§mta huvudinneh√•ll
            main_text = extract_main_content(soup)
            result['scraped_text'] = main_text

            if main_text:
                word_count = len(main_text.split())
                print(f"   ‚úì Lyckades! Skrapade {word_count} ord")
                if meta_desc:
                    print(f"   ‚úì Meta description: {meta_desc[:80]}...")
                result['status'] = 'success'
            else:
                print(f"   ‚ö† Lyckades bes√∂ka men ingen text hittades")
                result['status'] = 'no_content'

        elif response.status_code == 403:
            print(f"   ‚úó √Ötkomst nekad (403 Forbidden)")
            result['status'] = 'forbidden'

        elif response.status_code == 404:
            print(f"   ‚úó Sidan hittades inte (404)")
            result['status'] = 'not_found'

        else:
            print(f"   ‚úó HTTP {response.status_code}")
            result['status'] = f'http_{response.status_code}'

    except requests.exceptions.SSLError as e:
        print(f"   ‚úó SSL-fel, f√∂rs√∂ker med HTTP...")
        # F√∂rs√∂k med HTTP ist√§llet
        try:
            http_url = url.replace('https://', 'http://')
            response = requests.get(
                http_url,
                headers=HEADERS,
                timeout=HTTP_TIMEOUT,
                allow_redirects=True
            )

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                result['meta_description'] = extract_meta_description(soup)
                result['scraped_text'] = extract_main_content(soup)
                result['status'] = 'success_http'
                print(f"   ‚úì Lyckades med HTTP!")
            else:
                result['status'] = 'ssl_error_http_failed'
        except Exception:
            result['status'] = 'ssl_error'

    except requests.exceptions.Timeout:
        print(f"   ‚úó Timeout efter {HTTP_TIMEOUT}s")
        result['status'] = 'timeout'

    except requests.exceptions.ConnectionError:
        print(f"   ‚úó Anslutningsfel")
        result['status'] = 'connection_error'

    except Exception as e:
        print(f"   ‚úó Fel: {type(e).__name__}")
        result['status'] = f'error_{type(e).__name__}'

    return result


def main():
    parser = argparse.ArgumentParser(description='Skrapa text fr√•n f√∂retagshemsidor')
    parser.add_argument('--db', default='databases/ai_companies.db', help='S√∂kv√§g till databas')
    parser.add_argument('--limit', type=int, help='Begr√§nsa antal f√∂retag att skrapa (f√∂r test)')
    parser.add_argument('--missing-only', action='store_true', help='Bara f√∂retag utan description')
    parser.add_argument('--output', default='results/scraped_websites.csv', help='Output CSV-fil')
    parser.add_argument('--delay', type=float, default=1.0, help='F√∂rdr√∂jning mellan requests (sekunder)')
    args = parser.parse_args()

    # Skapa results-mapp om den inte finns
    Path(args.output).parent.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("üï∑Ô∏è  WEB SCRAPER - F√ñRETAGSHEMSIDOR")
    print("=" * 70)

    # Anslut till databas
    print(f"\nüìÇ L√§ser fr√•n databas: {args.db}")
    conn = sqlite3.connect(args.db)
    cursor = conn.cursor()

    # Bygg query
    if args.missing_only:
        query = """
            SELECT id, name, website, type
            FROM companies
            WHERE website IS NOT NULL
            AND website != ''
            AND (description IS NULL OR description = '')
            ORDER BY id
        """
        print("üéØ Filtrerar: Bara f√∂retag utan description")
    else:
        query = """
            SELECT id, name, website, type
            FROM companies
            WHERE website IS NOT NULL
            AND website != ''
            ORDER BY id
        """
        print("üéØ Skrapar: Alla f√∂retag med hemsida")

    if args.limit:
        query += f" LIMIT {args.limit}"

    cursor.execute(query)
    companies = cursor.fetchall()

    print(f"‚úì Hittade {len(companies)} f√∂retag att skrapa")

    if args.limit:
        print(f"‚ö† TESTL√ÑGE: Begr√§nsat till {args.limit} f√∂retag")

    print(f"‚è±Ô∏è  F√∂rdr√∂jning mellan requests: {args.delay}s")

    # Skrapa hemsidor
    results = []
    success_count = 0

    for i, (company_id, name, website, company_type) in enumerate(companies, 1):
        print(f"\n[{i}/{len(companies)}] " + "=" * 60)
        print(f"ID: {company_id} | {name}")
        print(f"Typ: {company_type} | Website: {website}")

        scrape_result = scrape_website(website, name)

        results.append({
            'id': company_id,
            'name': name,
            'website': website,
            'type': company_type,
            'scraped_text': scrape_result['scraped_text'],
            'meta_description': scrape_result['meta_description'] or '',
            'status': scrape_result['status'],
            'status_code': scrape_result['status_code'] or ''
        })

        if scrape_result['status'] in ['success', 'success_http']:
            success_count += 1

        # V√§nta lite mellan requests f√∂r att vara artig
        if i < len(companies):
            time.sleep(args.delay)

    # Exportera resultat
    print("\n" + "=" * 70)
    print("üìä EXPORTERAR RESULTAT")
    print("=" * 70)

    with open(args.output, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['id', 'name', 'website', 'type', 'scraped_text', 'meta_description', 'status', 'status_code']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    # Statistik
    failed = len(results) - success_count
    success_rate = (success_count / len(results) * 100) if results else 0

    print(f"\n‚úÖ Exporterat till: {args.output}")
    print(f"\nüìà RESULTAT:")
    print(f"   ‚úì Lyckade skrapningar: {success_count}")
    print(f"   ‚úó Misslyckade: {failed}")
    print(f"   üìä Total: {len(results)}")
    print(f"   üéØ Framg√•ngsgrad: {success_rate:.1f}%")

    # Status breakdown
    status_counts = {}
    for result in results:
        status = result['status']
        status_counts[status] = status_counts.get(status, 0) + 1

    if len(status_counts) > 1:
        print(f"\nüìã STATUS BREAKDOWN:")
        for status, count in sorted(status_counts.items(), key=lambda x: -x[1]):
            print(f"   {status}: {count}")

    conn.close()

    print("\n" + "=" * 70)
    print("‚úì KLART!")
    print("=" * 70)
    print(f"\nüí° N√§sta steg: Ladda upp {args.output} och k√∂r generate_descriptions.py")


if __name__ == '__main__':
    main()
